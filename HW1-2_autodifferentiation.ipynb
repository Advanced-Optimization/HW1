{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e605dc-2a52-4ac4-8d10-14307241b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Fix the random seed to facilitate grading\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089fceb-2ea7-4c5e-b9f5-5bc3234ecc65",
   "metadata": {},
   "source": [
    "# HW1.2 - Autodifferentiation\n",
    "\n",
    "In the previous notebook, we learned about some fundamental concepts for autodifferentiation; in particular \n",
    "- the computational benefit of seeing gradients as linear maps\n",
    "- the computational benefit of using the adjoint operator for computing gradients.\n",
    "\n",
    "Now, we will use these concepts in larger, more complex computation graphs. The goal is to better understand the computational benefits of the different approaches. \n",
    "\n",
    "In the previous notebook, you have already implemented several functions that are part of a standard neural network architecture. In the last part of the notebook, you exported the relevant tasks into their own module, making it easy to use these functions now.\n",
    "\n",
    "## 2.a Forward-mode Autodifferentiation w.r.t. Inputs. (20 pts)\n",
    "\n",
    "For pedagogical reasons, we first consider the following neural network. \n",
    "\n",
    "$$\n",
    "y = f(x, W_1, W_2, W_3) = f_3(f_2(f_1(x, W_1), W_2), W_3) = W_3 W_2 W_1 x\n",
    "$$\n",
    "\n",
    "In other words, we concatenate three times the linear map that we implemented in the previous notebook. \n",
    "\n",
    "We solve a regression problem using square error loss\n",
    "\n",
    "$$\n",
    "L(x, z) = \\| f(x) - z \\|^2\n",
    "$$\n",
    "\n",
    "where $z$ are target data points from the dataset. At first, we minimize the loss w.r.t. the input variables $x$ (not the weights). To do this, we need a couple of ingredients. \n",
    "\n",
    "**2.a.1** (6 pts)\n",
    "- Implement the linear map of the function f(x), returning both $d/dx$ and $d/dW$.\n",
    "- Implement the loss and its gradient with respect to $y$. \n",
    "- Implement the directional derivative of the loss at $y$ in the direction $dy$ via linear map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40677ca9-42a7-40d7-814a-02217624a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytorch.layers import linear\n",
    "\n",
    "### ============= 2.a.1 Fill in below ==============\n",
    "\n",
    "def linear_lmap(x, W, dx=None, dW=None):\n",
    "    ### Implement the linear map of the function at x or W in the direction dx or dW\n",
    "    ### return d/dx and d/dW\n",
    "    \n",
    "    lmap_x = None      # d/dx \n",
    "    lmap_W = None      # d/dW\n",
    "    return lmap_x, lmap_W\n",
    "\n",
    "def sqe(y, z):\n",
    "    ### Implement the sqe loss\n",
    "    \n",
    "    return None\n",
    "\n",
    "def sqe_grad(y, z):\n",
    "    ### Implement the gradient of the loss w.r.t. y\n",
    "\n",
    "    return None\n",
    "\n",
    "def sqe_lmap(y, dy, z):\n",
    "    ### Implement the directional derivative of the loss at y in the direction dy via a linear map\n",
    "    \n",
    "    return None\n",
    "\n",
    "### ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453f12b-f0b7-4524-946f-416d9653f144",
   "metadata": {},
   "source": [
    "Let us create a random instance of this forward network and calculate the loss. The input variable x has shape $\\mathbb{R}^{p \\times N}$, where $p$ is the input dimension and $N$ is the number of data points. To keep things simple, we first consider the single-input case with $N = 1$. We will extend this to the multiple-input (batch) case later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37793b7-234a-4d2f-9fb5-44dbb4b65f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1, m2, m3, p = 3, 5, 1, 2\n",
    "\n",
    "# We start with just one datapoint\n",
    "N = 1\n",
    "\n",
    "W_list = [np.random.rand(mi, mj) for mi, mj in zip([m1, m2, m3], [p, m1, m2])]\n",
    "x = np.random.rand(p, N) * 2 * np.pi\n",
    "\n",
    "z = np.random.rand(m3, N)\n",
    "y = linear(linear(linear(x, W_list[0]), W_list[1]), W_list[2])\n",
    "\n",
    "loss = sqe(y, z)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754f653-3284-4aaf-a909-641c23d83e77",
   "metadata": {},
   "source": [
    "**2.a.2** Using the above computation chain, implement the forward pass to calculate the directional derivative of the loss at with respect to $x$ in a randomly chosen direction $u$. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a7332-5bdf-42f2-9316-e57622489f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random direction with the same shape as x\n",
    "u = np.random.randn(x.size).reshape(x.shape)\n",
    "u = u / np.linalg.norm(u)\n",
    "\n",
    "### ============= 2.a.2 Fill in below ==============\n",
    "### Compute the directional derivative of the loss at x in the above random direction u.\n",
    "\n",
    "# forward pass\n",
    "s_0 = x\n",
    "s_1 = None\n",
    "s_2 = None\n",
    "s_3 = None\n",
    "\n",
    "# create a variation direction \n",
    "t_0 = u\n",
    "t_1 = None\n",
    "t_2 = None\n",
    "t_3 = None\n",
    "t_4 = None\n",
    "\n",
    "### ===========================\n",
    "print(t_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483d908-33d0-4881-8d94-bcd34f408807",
   "metadata": {},
   "source": [
    "**2.a.3** Compute the gradient of the loss with respect to x using the linear map, evaluated at $p$ canonical directions, and ensure it is the same as the analytical gradient. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bad7d-e8d5-4bc2-93f9-76c628ff561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_forward = np.zeros((p, 1))\n",
    "\n",
    "### ============= 2.a.3 Fill in below ==============\n",
    "### Do a forward pass in multiple unit directions to construct the gradient of loss function respect to x. Compare it with the analytical gradient.\n",
    "### Compute:\n",
    "### - grad_forward: the gradient of the loss function w.r.t x. computed via linear maps\n",
    "### - grad_analytical: the analytical gradient of the loss function w.r.t x.\n",
    "\n",
    "# the gradient of the loss function via linear maps\n",
    "for i in range(p):\n",
    "    grad_forward[i] = None\n",
    "\n",
    "# analytical gradient\n",
    "grad_analytical = None\n",
    "\n",
    "### ===========================\n",
    "\n",
    "np.testing.assert_allclose(grad_analytical, grad_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7b1b2-f716-40db-8cab-701807a94412",
   "metadata": {},
   "source": [
    "**2.a.4** What are two suboptimal aspects of the above implementation?  (4 pts)\n",
    "\n",
    "Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e4924-35e9-42fa-84a3-da3b377ee138",
   "metadata": {},
   "source": [
    "## 2.b Reverse-mode Autodifferentiation w.r.t. Inputs (7 pts)\n",
    "\n",
    "In the previous section, we have seen that computing the gradient using forward-mode differentiation is cumbersome. Following what you have seen in class, compute the gradient using the adjoint operator and the backpropagation algorithm (reverse-mode autodifferentiation). \n",
    "\n",
    "**2.b.1** Implement the adjoint operator of the linear map of the function f(x). (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2356c4d-ef4c-4c50-8b20-e5deb7b11a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 2.b.1 Fill in below ==============\n",
    "def linear_lmap_adjoint(x, W, du):\n",
    "    ### Implement the adjoint linear map of the function f(x) at x or W in the direction du\n",
    "    ### return du/dx, du/dW\n",
    "    \n",
    "    lmap_adjoint_x = None      # du/dx \n",
    "    lmap_adjoint_W = None      # du/dW\n",
    "    return lmap_adjoint_x, lmap_adjoint_W\n",
    "\n",
    "### ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61c24a-f5b3-49a1-ba0b-93e4e1852566",
   "metadata": {},
   "source": [
    "**2.b.2** Implement the forward and backward pass of the computation chain to compute the gradient of the loss with respect to input x. (5 pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487f027-a864-497c-a0a9-ea7824005699",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 2.b.2 Fill in below ==============\n",
    "### Do forward and backward passes to compute the gradients of loss with respect to x.\n",
    "### Compute:\n",
    "### - grad_reverse: the gradient of loss function w.r.t x.\n",
    "\n",
    "grad_reverse = None\n",
    "### ===========================\n",
    "\n",
    "np.testing.assert_allclose(grad_analytical, grad_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1da84-e20f-42cd-a01a-9fb79bc518da",
   "metadata": {},
   "source": [
    "## 2.c Autodifferentiation w.r.t. Network Weights (30 pts)\n",
    "\n",
    "So far, we have performed differentiation with respect to the inputs $x$. This is interesting when we solve optimal control or estimation problems, for example: $f(x)$ could represent our measurement or dynamics model, and we try to find $u$ to minimize the difference between a target and the outcome. \n",
    "\n",
    "In machine learning, a more common task is to tune the network parameters in order to create a predictor based on given training data. In this part of the notebook, we adopt the above pipeline to neural network training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174b095-c064-43e9-9f71-eba0c2dcd627",
   "metadata": {},
   "source": [
    "Below, we compute the analytical gradients of the cost w.r.t. $W_0$, $W_1$, and $W_2$. That way you can ensure that you have\n",
    "no mistakes moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07c241-16c4-42f1-af70-b84455a64800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below uses hard-coded gradients, for simplicity\n",
    "# dL/dAk=[2A_{k+1}'A_{k+2}'...A_{K}'(y^−y)]⋅[A_{k−1} … A_1 x]⊤\n",
    "yhat = W_list[2] @ W_list[1] @ W_list[0] @ x\n",
    "grad_W0_analytical = 2 * W_list[1].T @ W_list[2].T @ (yhat - z) @ x.T\n",
    "grad_W1_analytical = 2 * W_list[2].T @ (yhat - z) @ (W_list[0] @ x).T\n",
    "grad_W2_analytical = 2 * (yhat - z) @ (W_list[1] @ W_list[0] @ x).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0248a34-6ec0-4342-8d2f-2321b1fc5d30",
   "metadata": {},
   "source": [
    "**2.c.1** Below, calculate the gradient w.r.t. the network weights $W_k$ using a linear map. As in **1.d.1**, we first do this the \"inefficient way\" so that we see the benefit of backpropagation. (5 pts) \n",
    "\n",
    "(Hint: A perturbation in $W_k$ only affects layer $k$ and subsequent layers, so the forward-mode propagation should start at layer $k$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4106c36-b400-4904-850f-7a5f408be65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "    \n",
    "k = 1  # We want gradient w.r.t W_list[k]\n",
    "W = W_list[k]\n",
    "grad_Wk_forward = np.zeros_like(W)\n",
    "\n",
    "s_list = [s_0, s_1, s_2, s_3]\n",
    "\n",
    "for m, n in itertools.product(range(W.shape[0]), range(W.shape[1])):\n",
    "    V = np.zeros_like(W)\n",
    "    V[m, n] = 1.0\n",
    "\n",
    "    ### ============= 2.c.1 Fill in below ==============\n",
    "    ### Populate the gradient element (m,n) using the forward pass.\n",
    "    ### Compute:\n",
    "    ### - grad_Wk_forward: the gradient of loss function w.r.t W_list[k] \n",
    "    \n",
    "    grad_Wk_forward[m, n] = None\n",
    "    \n",
    "    ### ===========================\n",
    "if k == 0:\n",
    "    np.testing.assert_allclose(grad_Wk_forward, grad_W0_analytical)\n",
    "elif k == 1:\n",
    "    np.testing.assert_allclose(grad_Wk_forward, grad_W1_analytical)\n",
    "elif k == 2:\n",
    "    np.testing.assert_allclose(grad_Wk_forward, grad_W2_analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a8bb6-d18e-4e45-850a-4a96425f2046",
   "metadata": {},
   "source": [
    "**2.c.2** Now, compute the gradient of loss function w.r.t. the network weights $W_k$ using backpropagation. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef3400-acd6-47cc-91d6-bcea521587a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 2.c.2 Fill in below ==============\n",
    "### Compute the gradient of the loss function w.r.t. the network weights $W_k$ using backpropagation.\n",
    "### Compute:\n",
    "### - grad_Wk_reverse: the gradient of loss function w.r.t W_list[k] using backpropagation.\n",
    "\n",
    "grad_Wk_reverse = None\n",
    "### ===========================\n",
    "if k == 0:\n",
    "    np.testing.assert_allclose(grad_Wk_reverse, grad_W0_analytical)\n",
    "elif k == 1:\n",
    "    np.testing.assert_allclose(grad_Wk_reverse, grad_W1_analytical)\n",
    "elif k == 2:\n",
    "    np.testing.assert_allclose(grad_Wk_reverse, grad_W2_analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b87c36-5e05-4cb3-905d-a345fdae8164",
   "metadata": {},
   "source": [
    "## Adding nonlinearities\n",
    "\n",
    "Hopefully you find the above example useful to get a grasp of how autodifferentiation works, and to debug your code. Now let's move to more realistic examples so that you can actually train your own small neural network. The main missing elements, compared to standard neural networks, are biases and nonlinearities. For now, let's just add nonlinearities and see how far that can get us. \n",
    "\n",
    "As we have seen, to add nonlinearities, all we need is to define the derivative and the adjoint function, both defined as linear maps. Let's do this for two standard nonlinearities. \n",
    "\n",
    "**2.c.3** Fill in the lmap and lmap_adjoint for the ReLu and the sigmoid activation functions below. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feccad64-290a-48c7-afb0-b2b5afd7817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "def relu(x):\n",
    "    output = np.zeros_like(x)\n",
    "    output[x >= 0] = x[x >= 0]\n",
    "    return output\n",
    "\n",
    "### ============= 2.c.3 Fill in below ==============\n",
    "def relu_lmap(x, v):\n",
    "    ### Implement the linear map for ReLU at x in the direction v\n",
    "    ### by convention, set the gradient at zero to zero. \n",
    "    \n",
    "    return None\n",
    "\n",
    "def relu_lmap_adjoint(x, u):\n",
    "    ### Implement the adjoint linear map of the ReLU at x in the direction u\n",
    "    \n",
    "    return None\n",
    "### ===========================\n",
    "\n",
    "\n",
    "x = np.linspace(-1, 1, 100)\n",
    "v = np.ones_like(x)\n",
    "plt.figure()\n",
    "plt.plot(x, relu(x))\n",
    "plt.plot(x, relu_lmap(x, v))\n",
    "plt.xlabel(\"x\")\n",
    "plt.title(\"ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00221386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "### ============= 2.c.3 Fill in below ==============\n",
    "def sigmoid_lmap(x, v):\n",
    "    ### Implement the linear map for sigmoid at x in the direction v\n",
    "\n",
    "    return None\n",
    "\n",
    "def sigmoid_lmap_adjoint(x, u):\n",
    "    ### Implement adjoint map of the sigmoid at x in direction u\n",
    "    \n",
    "    return None\n",
    "### ===========================\n",
    "\n",
    "\n",
    "x = np.linspace(-np.pi, np.pi, 100)\n",
    "v = np.ones_like(x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.plot(x, sigmoid_lmap(x, v))\n",
    "plt.xlabel(\"x\")\n",
    "plt.title(\"Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab6d6a",
   "metadata": {},
   "source": [
    "Now we have all the ingredients to create our own little neural networks. \n",
    "\n",
    "For example, let's create the following neural network: Two layers of 128 dimensions, sigmoid activation functions, mean squared error (MSE) as a loss function. The network takes a batch of input $x \\in \\mathbb{R}^{3 * N}$, the produces outputs $y \\in \\mathbb{R}^{4 * N}$, where $N$ is the batch size. We will use this network in the next practical to map from end effector positions in 3D to the four motor commands for our robot example. \n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{3 \\times N} \\rightarrow \\mathrm{linear}(W_1) \\rightarrow sigmoid \\rightarrow \\mathrm{linear}(W_2) \\rightarrow sigmoid \\rightarrow \\mathrm{linear}(W_3) = y \\in \\mathbb{R}^{4 \\times N}\n",
    "$$\n",
    "\n",
    "To train the network, we use the mean squared error (MSE) loss:\n",
    "$$\n",
    "L(y,z) = \\frac{1}{4N}\\,\\lVert y - z \\rVert_F^2,\n",
    "\\qquad y, z \\in \\mathbb{R}^{4 \\times N}.\n",
    "$$\n",
    "\n",
    "Here, $\\lVert \\rVert_F$ denotes the Frobenius norm, and the loss averages the squared error over both the output dimensions and the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b26ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytorch.layers import linear\n",
    "\n",
    "m0 = 3\n",
    "m1, n1 = 128, m0\n",
    "m2, n2 = 128, m1\n",
    "m3, n3 = 4, m2\n",
    "\n",
    "W_list = [np.random.rand(mi, mj) for mi, mj in zip([m1, m2, m3], [n1, n2, n3])]\n",
    "\n",
    "N = 100\n",
    "x = np.random.rand(m0, N)\n",
    "z = np.random.rand(4, N)\n",
    "print(x.shape, z.shape)\n",
    "\n",
    "forward = (linear, sigmoid, linear, sigmoid, linear) \n",
    "W_for_layer = (W_list[0], None, W_list[1], None, W_list[2])\n",
    "lmap = (linear_lmap, sigmoid_lmap, linear_lmap, sigmoid_lmap, linear_lmap)\n",
    "lmap_adjoint = (linear_lmap_adjoint, sigmoid_lmap_adjoint, linear_lmap_adjoint, sigmoid_lmap_adjoint, linear_lmap_adjoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d232e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the analytical gradient for W_list[0], W_list[1], W_list[2] to ensure that you implement the correct gradient.\n",
    "s1 = W_list[0] @ x\n",
    "s2 = sigmoid(s1)\n",
    "s3 = W_list[1] @ s2\n",
    "s4 = sigmoid(s3)\n",
    "y = W_list[2] @ s4\n",
    "\n",
    "# mse loss\n",
    "loss = np.mean((y-z)**2)\n",
    "\n",
    "# backward (batch)\n",
    "delta5 = 2 * (y - z) / y.size                         # dL/dy, (4, B)\n",
    "grad_W2_analytical = delta5 @ s4.T                    # (4,B)@(B,128) -> (4,128)\n",
    "delta4 = (W_list[2].T @ delta5) * s4 * (1 - s4)       # (128,B)\n",
    "grad_W1_analytical = delta4 @ s2.T                    # (128,B)@(B,128) -> (128,128)\n",
    "delta2 = (W_list[1].T @ delta4) * s2 * (1 - s2)       # (128,B)\n",
    "grad_W0_analytical = delta2 @ x.T                     # (128,B)@(B,3) -> (128,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f0d39",
   "metadata": {},
   "source": [
    "**2.c.4** Compute the gradients w.r.t. the network parameters $W_k$ using backpropagation. Fill in the missing parts in the forward and backward pass (10 pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_grad(x_batch, z_batch, W_for_layer, forward, lmap_adjoint):\n",
    "    ### ============= 2.c.4 Fill in below ==============\n",
    "    ### Do forward and backward passes to the gradient w.r.t. the network weights $W_k$ using backpropagation.\n",
    "    ### Fill in the missing parts in the forward and backward pass\n",
    "    \n",
    "    # Forward pass \n",
    "    s_list = [x_batch]\n",
    "\n",
    "    for fun, W in zip(forward, W_for_layer):\n",
    "        s = None                         # (forward propagate through layer)\n",
    "        s_list.append(s)\n",
    "\n",
    "    y = s_list[-1]\n",
    "\n",
    "    # Loss\n",
    "    loss = np.mean((y - z_batch) ** 2)\n",
    "\n",
    "    # Backward pass\n",
    "    r = None                             # (gradient of MSE loss w.r.t. y)\n",
    "    rw_list = []\n",
    "\n",
    "    for func, W, s in zip(reversed(lmap_adjoint), reversed(W_for_layer), reversed(s_list[:-1])):\n",
    "        r, rw = None                     # (backward propagate through linear layer)\n",
    "        rw_list.insert(0, rw)\n",
    "\n",
    "    return loss, rw_list\n",
    "### ===========================\n",
    "loss, rW_list = compute_loss_and_grad(x, z, W_for_layer, forward, lmap_adjoint)\n",
    "\n",
    "np.testing.assert_allclose(rW_list[0], grad_W0_analytical)  \n",
    "np.testing.assert_allclose(rW_list[1], grad_W1_analytical)  \n",
    "np.testing.assert_allclose(rW_list[2], grad_W2_analytical)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b62977",
   "metadata": {},
   "source": [
    "**2.c.5** Implement the same network using pytorch and ensure that the gradients are the same. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88acccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float64\n",
    "W_torch = []\n",
    "\n",
    "# define the pytorch parameters from numpy\n",
    "W_0 = torch.tensor(W_list[0], dtype=dtype, requires_grad=True)\n",
    "W_1 = torch.tensor(W_list[1], dtype=dtype, requires_grad=True)\n",
    "W_2 = torch.tensor(W_list[2], dtype=dtype, requires_grad=True)\n",
    "xt = torch.tensor(x, dtype=dtype)\n",
    "zt = torch.tensor(z, dtype=dtype)\n",
    "\n",
    "### ============= 2.c.5 Fill in below ==============\n",
    "### Use pytorch to compute the gradient w.r.t. the network weights $W_k$.\n",
    "### - W_k.grad.numpy(): the gradient of loss function w.r.t W_list[k] \n",
    "\n",
    "\n",
    "### ===========================\n",
    "\n",
    "np.testing.assert_allclose(W_0.grad.numpy(), rW_list[0])\n",
    "np.testing.assert_allclose(W_1.grad.numpy(), rW_list[1])\n",
    "np.testing.assert_allclose(W_2.grad.numpy(), rW_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4ed2b-4ef9-426f-acae-ba2fedf446dc",
   "metadata": {},
   "source": [
    "# 2.d Packaging of functions (10 pts)\n",
    "\n",
    "**2.d.1** As in the last notebook, make sure to package some of this code so that you can reuse it in the next notebook. You will need the linear layers, the activation functions, and the squared error loss. Move the code to these modules and write corresponding tests. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638b341-fe47-4e8c-8e1d-4caaf7b74f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest mytorch/tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa129f11-2296-479b-9ea3-90706d22450b",
   "metadata": {},
   "source": [
    "**2.d.2** Answer the questions below about the package that you created. Note that there is no right or wrong; these questions serve to make you reflect about what you have implemented. (5 pts)\n",
    "- What tests did you implement? \n",
    "- Are you sure that your code works based on these tests? \n",
    "- Is the interface to your code straightforward (i.e., how many lines of code are required to run your tests?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9864be",
   "metadata": {},
   "source": [
    "## Acknowledgment of Collaboration and/or Tool Use\n",
    "\n",
    "Please choose from below (simply delete the lines that do not apply) and add a few additional notes\n",
    "\n",
    "- “I worked alone on this assignment.”, or\n",
    "- “I worked with ~~~~~~ [person or tool] on this assignment.” and/or\n",
    "- “I received assistance from ~~~~~~ [person or tool] on this assignment.”\n",
    "\n",
    "For the last two cases, specify how the person or tool helped you and explain why this amplified your learning process:\n",
    "\n",
    "_add answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e106927-8da9-4dfd-aa8a-4c53e78cc278",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Elements of Differentiable Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
