{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e6485-2d54-46df-9871-9ea8ae10b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Fix the random seed to facilitate grading\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7d468-d1f8-429b-99d7-d1568ae54527",
   "metadata": {},
   "source": [
    "# HW1.1 - essentials of differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa00ec6-4b6f-4b15-9310-6dbaf5cbbc1a",
   "metadata": {},
   "source": [
    "## 1.a Finite-difference tests (11 pts) \n",
    "\n",
    "Autodifferentiation allows us to evaluate the derivative of $f$ at any point $x$. To test what we will implement below, we review a very common sanity test in mathematical programming: the finite-difference test. We can use it to verify, using essentially the definition of the derivative, that our gradient implementation is correct. \n",
    "\n",
    "Using the skeleton below to fill in the missing code, run the cell to confirm it executes without errors. Then, answer the follow-up questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd4673",
   "metadata": {},
   "source": [
    "**1.a.1** Implement direction-wise forward-differences test (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187b429-60b2-4165-b218-9b749723e8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + np.log(1 + x) - np.sin(x)\n",
    "\n",
    "def f_grad(x):\n",
    "    return 2*x + 1/(1 + x) - np.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bb30b-1ee7-4269-af43-db6f6d6d129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(5)\n",
    "\n",
    "grad_f_ours = f_grad(x)\n",
    "eps = 1e-7\n",
    "\n",
    "for i in range(len(x)):\n",
    "    ### ============= 1.a.1 Fill in below ==============\n",
    "    ### Implement direction-wise forward-differences test\n",
    "    ### using x as the evaluation point. \n",
    "    ### Compute:\n",
    "    ### - grad_f_fd : gradient approximated using forward-difference method\n",
    "\n",
    "    grad_f_fd = None\n",
    "\n",
    "    ### ===========================\n",
    "\n",
    "    np.testing.assert_allclose(grad_f_fd[i] , grad_f_ours[i], atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28608b4b-6bf4-4125-93e9-c559a6d06386",
   "metadata": {},
   "source": [
    "### Follow-up questions: \n",
    "\n",
    "**1.a.2** The precision of the test above is not great. The tolerance `atol=1e-5` is relatively loose. How can we increase precision? (2 pts)\n",
    "\n",
    "Answer: \n",
    "\n",
    "**1.a.3** Let's say the function has `np.log(x + 0.001)` instead of `np.log(x + 1)`. Do you foresee any problems with this function? (2 pts)\n",
    "\n",
    "Answer: \n",
    "\n",
    "**1.a.4** The chosen delta and tolerance are a bit arbitrary. Do you have an idea for implementing this test in a more principled way? (Hint: use the error rate of the finite-difference gradient estimate) (2 pts)\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7d863-3643-4477-bc4e-9653935d6aff",
   "metadata": {},
   "source": [
    "## 1.b From gradients to linear maps (24pts)\n",
    "\n",
    "To prepare for automatic differentiation, we now reimplement the ideas above using **linear maps**. Recall that the **directional derivative** of a function can be viewed as a linear map. We have:\n",
    "\n",
    "$$\n",
    "\\partial f(x)[v] = \\lim_{\\delta \\rightarrow 0} \\frac{f(x+\\delta v) - f(x)}{\\delta}\n",
    "$$\n",
    "\n",
    "So, just like before, we can approximate this directional derivative using **finite difference** method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b407f",
   "metadata": {},
   "source": [
    "**1.b.1** Implement forward-difference test for the directional derivative (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b0823-a5c2-4072-946e-1804ce33bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_lmap(x, v): \n",
    "    assert len(x) == len(v)\n",
    "    return f_grad(x) * v\n",
    "\n",
    "x = np.ones(5)\n",
    "\n",
    "### ============= 1.b.1 Fill in below ==============\n",
    "### Implement forward-difference test for the directional derivative.\n",
    "### Use x as the evaluation point and sample a random direction v = np.random.rand(5)\n",
    "### Compute:\n",
    "### - grad_f_ours: directional derivative computed via the linear map\n",
    "### - grad_f_est : directional derivative approximated using finite differences method\n",
    "\n",
    "grad_f_ours = None\n",
    "grad_f_est = None\n",
    "\n",
    "### =========================== \n",
    "\n",
    "np.testing.assert_allclose(grad_f_ours, grad_f_est, rtol=1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f54a33-60dd-4e9b-8bb5-8bf5aaeee441",
   "metadata": {},
   "source": [
    "So far, we have focused on functions $f : \\mathbb{R}^P \\to \\mathbb{R}$, that take a vector as input and produce a scalar as output. However, functions that use **matrix or even tensor inputs/outputs** are commonplace in neural networks. In this example, let's consider the function that takes a matrix as input:\n",
    "\n",
    "$$\n",
    "y = f(W) = Wx, W\\in\\mathbb{R}^{m\\times n}\n",
    "$$\n",
    "\n",
    "You can think of this as a neural network with one layer and no activation functions. Here, $x \\in \\mathbb{R}^D$ is the input, $W \\in \\mathbb{R}^{M \\times D}$ are the weights that we try to learn, $y \\in \\mathbb{R}^M$ are the output labels. How would you differentiate $f(W)$ with respect to $W$? \n",
    "\n",
    "This is where linear maps start to shine. Answer the questions below to convince yourself of the utility of linear maps. Then, implement the code below. \n",
    "\n",
    "**1.b.2** What is the expression of $\\partial f(W)[V]$, the directional derivative of $f$ at $W \\in \\mathbb{R}^{M \\times D}$ in the direction of $V \\in \\mathbb{R}^{M \\times D}$? (5 pts)\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "**1.b.3** In the previous question, we expressed the directional derivative $\\partial f(W)[V]$ as a linear map. Now, we would like to express the same directional derivative using the standard gradient. Since the function takes a matrix as input, we introduce new variables: $w:=\\mathrm{vec}(W)$, $v:=\\mathrm{vec}(V)$. What is the expression of the standard gradient with respect to $w$?  (5 pts)\n",
    "\n",
    "Hint: You can calculate the gradient by massaging the expression of $\\partial f(W)[V]$ in the inner-product form $\\langle \\cdot, v \\rangle$, using the Kronecker–vec identity\n",
    "\n",
    "$$\n",
    "Vx = (I_m \\otimes x^\\top) \\mathrm{vec}(V)\n",
    "$$\n",
    "\n",
    "Answer:\n",
    "\n",
    "Next, implement the above to verify the math and to observe the differences! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b6895-87e0-4311-a2fd-f6cc0932f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, W):\n",
    "    return W @ x\n",
    "\n",
    "### ============= 1.b.2/1.b.3 Fill in below ==============\n",
    "### Implement the directional derivative and the gradients that you found above\n",
    "### Verify 1.b.2 and 1.b.3 using the provided code.\n",
    "\n",
    "def f_lmap(x, W, V):\n",
    "    # Implement the directional derivative via the linear map\n",
    "\n",
    "    return None\n",
    "\n",
    "def f_jac(x, W):\n",
    "    # Implement the Jacobian respect to vec(W)\n",
    "    # hint: Using the Kronecker product (np.kron)\n",
    "\n",
    "    return None\n",
    "### =========================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53058f3-acc7-4dcd-b3bb-434449149150",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 7\n",
    "p = 8\n",
    "W = np.random.rand(m, p)\n",
    "V = np.random.rand(m, p)\n",
    "\n",
    "x = np.random.rand(p)\n",
    "\n",
    "J = f_jac(x, W)\n",
    "\n",
    "plt.matshow(J)\n",
    "plt.show()\n",
    "\n",
    "lmap_naive = J @ V.flatten()\n",
    "lmap_better = f_lmap(x, W, V)\n",
    "\n",
    "np.testing.assert_allclose(lmap_naive, lmap_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94c33f",
   "metadata": {},
   "source": [
    "**1.b.4** Implement the forward-difference test for f_lmap (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00b180-b7df-4a8b-843d-a3dbb8e3f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 1.b.4 Fill in below ==============\n",
    "\n",
    "### Implement forward-difference test for f_lmap.\n",
    "### Compute:\n",
    "### - lmap_fd : directional derivative approximated using finite differences method\n",
    "### - lmap_ours: directional derivative computed via the linear map\n",
    "\n",
    "lmap_fd = None\n",
    "lmap_ours = None\n",
    "### ===========================\n",
    "\n",
    "np.testing.assert_allclose(lmap_fd, lmap_ours, atol=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f785169-30b0-4496-b96a-7f27d97f2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 100 -r 100 f_jac(x, W) @ V.flatten()\n",
    "%timeit -n 100 -r 100 f_lmap(x, W, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a94c11-2a2f-4087-8a79-9a9bae2ffaa9",
   "metadata": {},
   "source": [
    "**1.b.5** Comment on the following behavior. (4 pts) \n",
    "- Why does the forward-difference test pass to very high accuracy in this example?\n",
    "\n",
    "    Answer:\n",
    "\n",
    "- Why is f_grad significantly slower than f_lmap?   \n",
    "\n",
    "    Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd533bf-a7e7-4c25-aabe-f501b8face06",
   "metadata": {},
   "source": [
    "## 1.c Chain rule from three perspectives. (15 pts)\n",
    "\n",
    "Now that we understand the role of linear maps and gradients, we will start stitching together multiple operations. \n",
    "\n",
    "Let's define \n",
    "\n",
    "$$\n",
    "f(W) = Wx,\\quad g(u) = \\log \\sum_{j=1}^{M} e^{u_j},\n",
    "$$\n",
    "\n",
    "where $W \\in \\mathbb{R}^{M \\times D}$, function $g:\\mathbb{R}^M \\to \\mathbb{R}$ is the log-sum-exp operator. We consider the composite function\n",
    "\n",
    "$$\n",
    "L(W) = (g \\circ f)(W)\n",
    "$$ \n",
    "\n",
    "Our goal is to use three different methods to calculate the directional derivative $\\partial L(W)[V]$ based on the chain rule, as shown in the implementation below. \n",
    "\n",
    "**Write the mathematical expressions of the three different ways to compute the directional derivative via the chain rule. You may use the provided implementation for inspiration.**\n",
    "\n",
    "**1.c.1** Using the multiplication of the respective Jacobians. (5 pts)\n",
    "\n",
    "Answer:\n",
    "\n",
    "**1.c.2** Using standard linear map (as seen in forward-mode autodiff). (5 pts)\n",
    "\n",
    "Answer:\n",
    "\n",
    "**1.c.3** Using adjoints (see also reverse-mode autodiff). (5 pts)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b2be3-2741-4791-bdcf-e3bb9574afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_grad(y):\n",
    "    return np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "def g_lmap(y, v):\n",
    "    return g_grad(y) @ v\n",
    "\n",
    "def f_lmap(x, W, V):\n",
    "    # argument W is kept for consistency although not used here\n",
    "    return V @ x\n",
    "\n",
    "def f_lmap_adj(x, W, u):\n",
    "    # computes u x.T even when u, x are arrays (u @ x.T doesn't work in that case)\n",
    "    return np.outer(u, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e57ad5-6457-4728-b8e6-5f7fcfe311d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = f_jac(x, W)\n",
    "grad_concat = g_grad(f(x, W)) @ J\n",
    "grad_concat_V = grad_concat @ V.flatten()\n",
    "\n",
    "grad_lmap = g_lmap(f(x, W), f_lmap(x, W, V))\n",
    "\n",
    "grad_adjoint = f_lmap_adj(x, W, g_grad(f(x, W)))\n",
    "grad_adjoint_V = np.trace(grad_adjoint.T @ V)\n",
    "\n",
    "np.testing.assert_allclose(grad_concat_V, grad_lmap)\n",
    "np.testing.assert_allclose(grad_concat_V, grad_adjoint_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66a372-2bc9-4e16-8c45-0944dae4c8b6",
   "metadata": {},
   "source": [
    "## 1.d Towards forward-mode vs. reverse-mode autodifferentiation (20 pts)\n",
    "\n",
    "Finally, we want to show how easy it is to get the gradient from the adjoint linear map as opposed to the normal linear map. \n",
    "\n",
    "Below, implement two ways of calculating the derivative of $L(W)$ w.r.t. $W$:\n",
    "\n",
    "$$\n",
    "\\nabla_W L(W) \\in \\mathbb{R}^{m \\times p}\n",
    "$$\n",
    "\n",
    "**1.d.1** (10 pts) First, compute the gradient by perturbing the weights along each coordinate direction of weight matrix.\n",
    "Specifically, for each matrix entry $(i, j)$, define the direction\n",
    "\n",
    "$$\n",
    "V_{ij} \\in \\mathbb{R}^{m \\times p}\n",
    "\\quad \\text{with} \\quad\n",
    "(V_{ij})_{kl} =\n",
    "\\begin{cases}\n",
    "1, & (k,l) = (i,j), \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Using the chain rule with standard linear map, compute each entry of the gradient as\n",
    "$$\n",
    "(\\nabla_W L(W))_{ij} = \\partial L(W)[V_{ij}] = \\partial g(f(W))\\big[\\partial f(W)[V_{ij}]\\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e72420-5ba2-470a-888a-ca62211712f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 1.d.1 Fill in below ==============\n",
    "### Compute the gradient using forward-mode autodifferentiation\n",
    "\n",
    "grad_lmap_vec = None\n",
    "### ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833df98-8c0e-438e-bee4-e463f12362e9",
   "metadata": {},
   "source": [
    "**1.d.2** (10 pts) Second, use the adjoint operator to obtain the gradient vector in one step. Recall from the chain rule in adjoint form:\n",
    "\n",
    "$$\n",
    "\\nabla_{W}L(W) = \\partial f(W)^*\\left[ \\nabla g(f(W)) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40191ea5-b16f-4278-a8c4-afe6b67754d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 1.d.2 Fill in below ==============\n",
    "### Compute the gradient using reverse-mode autodifferentiation\n",
    "\n",
    "grad_adjoint_vec = None\n",
    "### ===========================\n",
    "\n",
    "np.testing.assert_allclose(grad_lmap_vec, grad_adjoint_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bd464-c0c9-40eb-87a7-65aa4ae6f8aa",
   "metadata": {},
   "source": [
    "# 1.e Packaging of functions (10 pts)\n",
    "\n",
    "So far, all the functions you wrote live only in this notebook. In this last part of this notebook, we will follow a few standard coding practices to package things in a more usable form for what's coming next. \n",
    "\n",
    "In particular, it is better practice to: \n",
    "- move functions that are ready to use to python modules (.py files).\n",
    "- create a local python package that we can easily import from all subfolders etc. to avoid having a mess with paths etc.\n",
    "- create appropriate unit tests. (eg. finite-difference test)\n",
    "\n",
    "**1.e.1** (5pts) Create a python package, for example ``mytorch''. Move the relevant functions to this package, and create appropriate tests. After doing this, you should be able to install the package using `pip install -e mytorch`. Then make sure you create some tests and show their outputs, as in the example below. One test that you should definitely be able to use is the finite-difference test that you created in the very beginning of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799a6fb-4c76-4c9f-bdab-eeaae50ea77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest mytorch/tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe50ea1-b4ce-4468-a41c-b431ec51f306",
   "metadata": {},
   "source": [
    "**1.e.2** (5 pts) Answer the questions below about the package that you created. Note that there is no right or wrong; these questions serve to make you reflect about what you have implemented. \n",
    "- What tests did you implement? \n",
    "- Are you sure that your code works based on these tests? \n",
    "- Is the interface to your code straightforward (i.e., how many lines of code are required to run your tests?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e9e89-80b8-4661-923b-2739daaa8831",
   "metadata": {},
   "source": [
    "## Acknowledgment of Collaboration and/or Tool Use\n",
    "\n",
    "Please choose from below (simply delete the lines that do not apply) and add a few additional notes\n",
    "\n",
    "- “I worked alone on this assignment.”, or\n",
    "- “I worked with ~~~~~~ [person or tool] on this assignment.” and/or\n",
    "- “I received assistance from ~~~~~~ [person or tool] on this assignment.”\n",
    "\n",
    "For the last two cases, specify how the person or tool helped you and explain why this amplified your learning process:\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4397e9b-3f04-415e-859a-4a46f228b2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
