{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb15624-d3fd-407c-a609-73d9a695bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Fix the random seed to facilitate grading\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b3d21-ce77-48be-a869-d3e422d17e22",
   "metadata": {},
   "source": [
    "# HW1.3 - gradient descent\n",
    "\n",
    "## 3.a Vanilla Gradient Descent (30 pts)\n",
    "\n",
    "We have spent quite some time understanding how automatic differentiation works. This is only one ingredient of the success story of neural networks. Another important ingredient are efficient optimization algorithms. In this final part of Homework 1, we will implement our own version of modern optimization algorithms for deep learning.\n",
    "\n",
    "We start with a very minimalistic version of gradient descent. We will eventually apply it to the neural network from the previous notebook. But to start with, let's use it on a simple yet rich problem for which we know the optimal solution.\n",
    "\n",
    "In particular, for some feature map $\\phi$ that we will introduce later, we will use the following standard nonlinear least squares cost function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mathbf{w}) &= \\frac{1}{2} \\sum_i (y_i - \\phi(x_i)^\\top \\mathbf{w})^2 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**3.a.1** We want to write above as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mathbf{w}) &= \\sum_i L_i(\\mathbf{w}) \\\\ &= \\sum_i \\frac{1}{2} \\mathbf{w}^\\top \\mathbf{Q}_i \\mathbf{w} + \\mathbf{b}_i^\\top \\mathbf{w} + c_i \n",
    "= \\frac{1}{2} \\mathbf{w}^\\top \\mathbf{Q} \\mathbf{w} + \\mathbf{b}^\\top \\mathbf{w} + c\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Find the expression of $L_i(\\mathbf{w})$, $\\mathbf{Q}_i$ and $\\mathbf{b}_i$, and $c_i$. (4 pts)\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "\n",
    "In what follows, we will do everything in two dimensions to make plotting easier. That means that we will use $\\mathbf{x}$ in two dimensions and $\\phi(\\mathbf{x})=\\mathbf{x}$, so that $\\mathbf{w}$ is two-dimensional too. In practice, a better choice for $\\phi$ would a vector of monomials up to a degree $d$, or other more expressive and better conditioned basis functions. \n",
    "\n",
    "**3.a.2** Implement the cost function $L(\\mathbf{w})$ and optimal solution $\\mathbf{w}^*$ of this problem. Also, implement a unit test based on the optimal cost. (6 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e6744-a667-43b6-916b-24280d69b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "def phi(x, degree=1):\n",
    "    # With degree=1, below corresponds simply to the feature function phi(x) = x.\n",
    "    # We use the polynomial features so that it is easy to experiment with degrees > 1.\n",
    "    features = sklearn.preprocessing.PolynomialFeatures(degree, include_bias=False)\n",
    "    return features.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2f6da-bb82-4ca3-9b7c-c9e477aa7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10\n",
    "n = 2\n",
    "\n",
    "w_gt = np.random.rand(n)\n",
    "\n",
    "x1 = np.random.normal(loc=0.0, scale=0.3, size=(n_data, 1))\n",
    "x2 = np.random.normal(loc=0.0, scale=2.0, size=(n_data, 1))\n",
    "x_input = np.hstack((x1, x2))\n",
    "\n",
    "Phi = phi(x_input)\n",
    "assert Phi.shape[1] == 2 \n",
    "\n",
    "y_output = Phi @ w_gt + np.random.normal(scale=0.01, size=Phi.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf002b-1675-4a17-85d7-fa5d80d89a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 3.a.2 Fill in below ==============\n",
    "### Implement the forms of the cost function and the optimal solution below.\n",
    "\n",
    "Q = Phi.T @ Phi\n",
    "b = - Phi.T @ y_output\n",
    "\n",
    "def cost(w):\n",
    "    ### Implement the cost function L(w).\n",
    "\n",
    "    return None\n",
    "\n",
    "### Implement the optimal solution w_opt.\n",
    "\n",
    "w_opt = None\n",
    "\n",
    "def test_cost_opt(w_opt):\n",
    "    ### Implement the unit test for optimal cost. Check that the cost at w_opt is less than the cost at random points.\n",
    "    \n",
    "    return None\n",
    "\n",
    "### ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c546a-98da-4e3b-a14a-9de0d12c3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cost_opt(w_opt)\n",
    "\n",
    "# for plotting only\n",
    "x = np.linspace(w_opt[0]-2, w_opt[0]+2, 200)\n",
    "y = np.linspace(w_opt[1]-1, w_opt[1]+1, 100)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "zz = np.array([cost(np.array([xi, yi])) for xi, yi in zip(xx.flatten(), yy.flatten())])\n",
    "\n",
    "def setup_plot(title=\"\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5)\n",
    "\n",
    "    ax.contourf(xx, yy, zz.reshape(xx.shape))\n",
    "    ax.scatter(*w_opt, color=\"white\", marker=\"x\")\n",
    "    ax.axis(\"equal\")\n",
    "    ax.set_title(title)\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = setup_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45807d48-8d0b-4f04-9794-313132108347",
   "metadata": {},
   "source": [
    "Now, we can perform gradient descent on this function. \n",
    "\n",
    "**3.a.3** Implement the directional derivative, and its adjoint (6 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21f79f-527a-478e-bc20-6f394916cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 3.a.3 Fill in below ==============\n",
    "def cost_lmap(w, u):\n",
    "    ### Implement the linear map of cost function L(w) at w in the direction of u.\n",
    "\n",
    "    return None\n",
    "    \n",
    "def cost_lmap_adjoint(w, v):\n",
    "    ### Implement the adjoint map of cost function L(w) at w in the direction of v.\n",
    "    \n",
    "    return None\n",
    "### ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d5b36-2cc9-43b8-ba3b-a47cbadbecec",
   "metadata": {},
   "source": [
    "**3.a.4** Implement the simplest form of gradient descent: with fixed stepsize alpha and fixed number of iterations. To keep the notation consistent with the textbook, we denote the optimization variable by $x$ instead of $w$ in the following implementation. (4 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b581a-a022-44ad-902f-856e75515012",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = setup_plot(\"Gradient descent iterates\")\n",
    "\n",
    "xi = np.array([-1.25, 0.0])\n",
    "\n",
    "max_iter = 100\n",
    "alpha = 0.02\n",
    "\n",
    "x_list_gd = [xi]\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    ax.scatter(*xi, color=\"white\", marker=\"o\")\n",
    "    \n",
    "    ### ============= 3.a.4 Fill in below ==============\n",
    "    ### Implement the gradient descent with fixed step size alpha and fixed number of iterations.\n",
    "    \n",
    "    xi = None\n",
    "    \n",
    "    ### ==============================\n",
    "    x_list_gd.append(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b110c-efa2-4571-bfab-d6b0a771ea5b",
   "metadata": {},
   "source": [
    "**3.a.5** Plot two different visualizations of the convergence rate and comment on the behavior. (10 pts)\n",
    "1. Plot the error  $e_k = \\left| f(x_k) - f(x^*) \\right|$ versus the iteration index $k$, using a logarithmic scale on the $e_k$ axis.\n",
    "2. Plot $e_{k+1}$ versus $e_k$ using log–log axes.  \n",
    "\n",
    "Based on these plots, answer the following questions:\n",
    "\n",
    "- What is the convergence rate from the plots? \n",
    "- What would the convergence be like if for example we had $Q=\\begin{bmatrix} 10^5 & 0 \\\\ 0 & 10^{-3} \\end{bmatrix}$? Feel free to temporarily change your code to observe what happens.\n",
    "- How to prediect the convergence rate based on the problem parameters?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b027c5f-6f2e-479d-8933-044e415f87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_rate(x_list):\n",
    "    C = 1.03\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 5)\n",
    "    \n",
    "    ### ============= 3.a.5 Fill in below ==============\n",
    "    ### Plot the two different visulizations of convergence rate.\n",
    "\n",
    "    errors = None\n",
    "    ### ==============================\n",
    "\n",
    "    axs[0].set_xlabel(\"iteration k\")\n",
    "    axs[0].set_ylabel(\"log error e[k]\")\n",
    "    \n",
    "    axs[1].set_xlabel(\"error e[k]\")\n",
    "    axs[1].set_ylabel(\"error e[k+1]\")\n",
    "    axs[1].axis(\"equal\")\n",
    "    [ax.grid(True) for ax in axs]\n",
    "    plt.show()\n",
    "\n",
    "plot_convergence_rate(x_list_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35723704-6965-44b2-8bf5-e4f5bd95f5cd",
   "metadata": {},
   "source": [
    "## 3.b Preconditioning (18 pts)\n",
    "\n",
    "One trick that helps with convergence is to use different stepsizes in different directions. Indeed, the larger the curvature is, the smaller of a stepsize we should choose. \n",
    "\n",
    "Preconditioning is a method to introduce a change of coordinates first, so that in the new variable, the problem is better conditioned. \n",
    "There are different forms of preconditioning, from the most effective, but also expensive one, to cheaper approximate alternatives . \n",
    "\n",
    "**3.b.1** Implement preconditioning using the exact Hessian and a fixed step size $\\alpha$ and answer the following questions: (10 pts)\n",
    "- How does this preconditioning update differ from the standard Newton method?\n",
    "- What happens when you set $\\alpha$ to one and why?\n",
    "- What is the new convergence rate? Justify mathematically. (Hint: Find the relationship between $e_{k+1}=x_{k+1}-x^\\star$  and $e_k=x_k-x^\\star$.)\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb06022-1be2-47dd-81c4-2b1f3ddf83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = setup_plot(\"Exact preconditioning iterates\")\n",
    "\n",
    "alpha = 0.1\n",
    "xi = np.array([-1.25, 0.0])\n",
    "\n",
    "x_list_pre = [xi]\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    ax.scatter(*xi, color=\"white\", marker=\"o\")\n",
    "\n",
    "    ### ============= 3.b.1 Fill in below ==============\n",
    "    ### Implement the preconditioning gradient descent using exact Hessian and a constant step size alpha.\n",
    "    \n",
    "    xi = None\n",
    "    \n",
    "    ### ==============================\n",
    "    \n",
    "    x_list_pre.append(xi)\n",
    "plot_convergence_rate(x_list_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1aa6b-188a-4f72-b84c-e080dbd86904",
   "metadata": {},
   "source": [
    "**3.b.2** Implement preconditioning using the accumulated squared gradient (as in AdaGrad). Notice that it does not seem to give an improvement. Explain why that might be. (8 pts)\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a320e-922b-46bc-86fa-1cef0a147c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = setup_plot(\"AdaGrad iterates\")\n",
    "\n",
    "xi = np.array([-1.25, 0.0])\n",
    "epsilon = 1e-5\n",
    "alpha = 0.1\n",
    "\n",
    "x_list_ada = [xi]\n",
    "\n",
    "curvature = np.zeros_like(xi)   \n",
    "for i in range(max_iter):\n",
    "    \n",
    "    ax.scatter(*xi, color=\"white\", marker=\"o\")\n",
    "\n",
    "    ### ============= 3.b.2 Fill in below ==============\n",
    "    ### Implement the AdaGrad update steps\n",
    "    \n",
    "    xi = None\n",
    "    \n",
    "    ### ==============================\n",
    "    \n",
    "    x_list_ada.append(xi)\n",
    "plot_convergence_rate(x_list_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fc74b-ddf1-41ee-a441-82d080c678c0",
   "metadata": {},
   "source": [
    "## 3.c Acceleration (27 pts)\n",
    "\n",
    "**3.c.1** Implement the Polyak momentum update. (4 pts)\n",
    "\n",
    "**3.c.2** Implement the Expected moving average momentum update. (4 pts)\n",
    "\n",
    "**3.c.3** Compare the behavior for $\\beta=0.2,0.5,0.9$. (3 pts)\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9d4f8-8f0c-45d9-bca3-57503f877b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = setup_plot()\n",
    "\n",
    "beta = 0.2 # play around with this, put it back to 0.2 when done. \n",
    "\n",
    "alpha = 0.02\n",
    "xi_poly = np.array([-1.25, 0.0])\n",
    "xi_ema = np.array([-1.25, 0.0])\n",
    "\n",
    "x_list_poly = [xi_poly]\n",
    "x_list_ema = [xi_ema]\n",
    "\n",
    "velocity_poly = np.zeros_like(xi_poly)\n",
    "\n",
    "velocity_ema = np.zeros_like(xi_poly)\n",
    "for i in range(max_iter):\n",
    "        \n",
    "    ax.scatter(*xi_poly, color=\"white\", marker=\"o\")\n",
    "    ax.scatter(*xi_ema, color=\"orange\", marker=\"o\")\n",
    "    \n",
    "    ### ============= 3.c.1 Fill in below ==============\n",
    "    ### Implement Polyak Momentum\n",
    "    \n",
    "    xi_poly = None\n",
    "    \n",
    "    ### ==============================\n",
    "\n",
    "    ### ============= 3.c.2 Fill in below ==============\n",
    "    ### Implement EMA Momentum\n",
    "    \n",
    "    xi_ema = None\n",
    "    \n",
    "    ### ==============================\n",
    "    \n",
    "    x_list_poly.append(xi_poly)\n",
    "    x_list_ema.append(xi_ema)\n",
    "\n",
    "plot_convergence_rate(x_list_poly)\n",
    "plot_convergence_rate(x_list_ema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666aed4-b2c0-4de2-9814-26b698f5b665",
   "metadata": {},
   "source": [
    "## Putting everything together: ADAM\n",
    "\n",
    "You have all the building blocks to write your own simplified version of the ADAM algorithm! \n",
    "\n",
    "**3.c.4** Implement the ADAM iterates below: (8 pts)\n",
    "\n",
    "- exponential moving average for keeping track of the curvature information\n",
    "- EMA momentum for acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05c44a-842d-4cf5-ba00-aab9cd40bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = setup_plot(\"simplified ADAM iterates\")\n",
    "\n",
    "xi = np.array([-1.25, 0.0])\n",
    "epsilon = 1e-5\n",
    "\n",
    "x_list_adam = [xi]\n",
    "\n",
    "beta_momentum = 0.5\n",
    "beta_curvature = 0.5\n",
    "alpha = 0.05\n",
    "\n",
    "curvature = np.zeros_like(xi)\n",
    "velocity = np.zeros_like(xi)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    ax.scatter(*xi, color=\"white\", marker=\"o\")\n",
    "\n",
    "    ### ============= 3.c.4 Fill in below ==============\n",
    "    ### Compute the ADAM update steps\n",
    "    \n",
    "    xi = None\n",
    "    \n",
    "    ### ==============================\n",
    "    \n",
    "    x_list_adam.append(xi)\n",
    "plot_convergence_rate(x_list_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824e979",
   "metadata": {},
   "source": [
    "**3.c.5** Can you tune the values of beta_momentum, beta_curvature, and alpha, to get good behavior? Report the best values you found and some lessons you learned along the way. (8 pts)\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d07898-baac-4082-b06e-b90dda03a37f",
   "metadata": {},
   "source": [
    "## 3.d Stochastic Gradient Descent\n",
    "\n",
    "**3.d.1** Implement stochastic gradient descent (4 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5714321-333b-4468-aabb-11e871c0a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = setup_plot()\n",
    "\n",
    "alpha = 0.1\n",
    "xi = np.array([-1.25, 0.0])\n",
    "\n",
    "max_iter = 50\n",
    "\n",
    "x_list_sgd = [xi]\n",
    "velocity = np.zeros_like(xi)\n",
    "for i in range(max_iter):\n",
    "\n",
    "    ax.scatter(*xi, color=\"white\", marker=\"o\")\n",
    "\n",
    "    perm = np.random.permutation(n_data) \n",
    "    for n in perm:\n",
    "        phi_n = Phi[n, :]\n",
    "        ### ============= 3.d.1 Fill in below ==============\n",
    "        ### Implement SGD\n",
    "        \n",
    "        xi = None\n",
    "        ### ==============================\n",
    "        ax.scatter(*xi, color=\"white\", marker=\".\")\n",
    "    \n",
    "        x_list_sgd.append(xi)\n",
    "\n",
    "plot_convergence_rate(x_list_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd996c-109e-4b93-b293-65d4cf38af09",
   "metadata": {},
   "source": [
    "## 3.e Application to Neural Networks\n",
    "\n",
    "**3.e.1** Given the following input data $X \\in \\mathbb{R}^{3 \\times N}$ and output data $Z \\in \\mathbb{R}^{4 \\times N}$, implement neural network training piepline using the architecture defined in **2.c.4**. Fill in the missing parts of the code below and train the network using mini-batch stochastic gradient descent (SGD). (10 pts)\n",
    "\n",
    "(Hint: Use the gradient calculation from **2.c.4**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb182d2-dc42-4605-87e7-a7114f19596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytorch.layers import sigmoid\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(N, noise_std=0.0):\n",
    "    # true weights\n",
    "    W0_true = np.random.randn(128, 3)\n",
    "    W1_true = np.random.randn(128, 128)\n",
    "    W2_true = np.random.randn(4, 128)\n",
    "\n",
    "    # inputs (column-batch)\n",
    "    X = np.random.randn(3, N)\n",
    "\n",
    "    # forward (batch)\n",
    "    h1 = sigmoid(W0_true @ X)        # (128, N)\n",
    "    h2 = sigmoid(W1_true @ h1)       # (128, N)\n",
    "    Z  = W2_true @ h2                # (4, N)\n",
    "\n",
    "    # add noise\n",
    "    if noise_std > 0:\n",
    "        Z = Z + noise_std * np.random.randn(*Z.shape)\n",
    "\n",
    "    return X, Z\n",
    "\n",
    "# Generate a training dataset\n",
    "X, Z = generate_data(10000, noise_std=0.1)\n",
    "print(X.shape, Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78998f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytorch.layers import linear, linear_lmap, linear_lmap_adjoint, sigmoid_lmap, sigmoid_lmap_adjoint\n",
    "\n",
    "m0 = 3\n",
    "m1, n1 = 128, m0\n",
    "m2, n2 = 128, m1\n",
    "m3, n3 = 4, m2\n",
    "\n",
    "W_list = [np.random.rand(mi, mj) for mi, mj in zip([m1, m2, m3], [n1, n2, n3])]\n",
    "\n",
    "forward = (linear, sigmoid, linear, sigmoid, linear) \n",
    "W_for_layer = (W_list[0], None, W_list[1], None, W_list[2])\n",
    "lmap = (linear_lmap, sigmoid_lmap, linear_lmap, sigmoid_lmap, linear_lmap)\n",
    "lmap_adjoint = (linear_lmap_adjoint, sigmoid_lmap_adjoint, linear_lmap_adjoint, sigmoid_lmap_adjoint, linear_lmap_adjoint)\n",
    "\n",
    "def compute_loss_and_grad(x_batch, z_batch, W_for_layer, forward, lmap_adjoint):\n",
    "    ### ============= Fill in below ==============\n",
    "    ### Copy your implementation from 2.c.4 here.\n",
    "    loss = None\n",
    "    rW_list = None\n",
    "    \n",
    "    return loss, rW_list\n",
    "\n",
    "### ===========================\n",
    "loss, rW_list = compute_loss_and_grad(X, Z, W_for_layer, forward, lmap_adjoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "\n",
    "# Store training loss for visualization\n",
    "loss_history = []\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 128\n",
    "\n",
    "# learning rate\n",
    "alpha = 1e-2\n",
    "\n",
    "N = X.shape[0]\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "num_batches = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    perm = rng.permutation(N)\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    ### ============= 3.e.1 Fill in below ==============\n",
    "    ### Implement neural network training piepline using mini-batch gradient descent.\n",
    "    ### Use the compute_loss_and_grad function to calculate loss and gradients.\n",
    "\n",
    "    epoch_loss = None\n",
    "    ### ===========================\n",
    "    # average over number of mini-batches\n",
    "    epoch_loss /= num_batches  \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | loss = {epoch_loss:.6f}\")\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "# Plot training loss\n",
    "plt.semilogy(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"MyTorch Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9464184",
   "metadata": {},
   "source": [
    "## 3.f Comparison with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0d5d3",
   "metadata": {},
   "source": [
    "**3.f.1** Reimplement the same neural network in PyTorch using mini-batch SGD. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "\n",
    "# Store training loss for visualization\n",
    "loss_history = []\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 128\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-2\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, 128, bias=False)\n",
    "        self.fc3 = nn.Linear(128, 4, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_t = torch.tensor(X.T, dtype=torch.float64)\n",
    "Z_t = torch.tensor(Z.T, dtype=torch.float64)\n",
    "\n",
    "# --- dataset / dataloader ---\n",
    "dataset = TensorDataset(X_t, Z_t)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "                    \n",
    "# Initialize the neural network\n",
    "net = SimpleNet().to(dtype=torch.float64)\n",
    "\n",
    "# Choose optimizer (SGD or Adam)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "# mse loss function\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    ### ============= 3.f.1 Fill in below ==============\n",
    "    ### Reimplement the same neural network in PyTorch using mini-batch SGD\n",
    "\n",
    "    epoch_loss = None\n",
    "    ### ==============================\n",
    "    # average over number of mini-batches\n",
    "    epoch_loss /= len(loader)\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | loss = {epoch_loss:.6f}\")\n",
    "        \n",
    "# --- plot ---\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Pytorch Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed968a",
   "metadata": {},
   "source": [
    "## Acknowledgment of Collaboration and/or Tool Use\n",
    "\n",
    "Please choose from below (simply delete the lines that do not apply) and add a few additional notes\n",
    "\n",
    "- “I worked alone on this assignment.”, or\n",
    "- “I worked with ~~~~~~ [person or tool] on this assignment.” and/or\n",
    "- “I received assistance from ~~~~~~ [person or tool] on this assignment.”\n",
    "\n",
    "For the last two cases, specify how the person or tool helped you and explain why this amplified your learning process:\n",
    "\n",
    "_add answer here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
